{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Unbiased Evaluation using a New Test Set\n",
    "\n",
    "In this part, we are given a new test set (`/dsa/data/all_datasets/back_order/Kaggle_Test_Dataset_v2.csv`). We can now take advantage of the entire smart sample that we created in Part I. \n",
    "\n",
    "* Retrain a pipeline using the optimal parameters that the pipeline learned. We don't need to repeat GridSearch here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import os, sys\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif, mutual_info_classif\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load smart sample and the best pipeline from Part II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "model3 = joblib.load('bestmodel.pkl')\n",
    "best_pipeline3 = model3.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>national_inv</th>\n",
       "      <th>lead_time</th>\n",
       "      <th>in_transit_qty</th>\n",
       "      <th>forecast_3_month</th>\n",
       "      <th>forecast_6_month</th>\n",
       "      <th>forecast_9_month</th>\n",
       "      <th>sales_1_month</th>\n",
       "      <th>sales_3_month</th>\n",
       "      <th>sales_6_month</th>\n",
       "      <th>sales_9_month</th>\n",
       "      <th>...</th>\n",
       "      <th>pieces_past_due</th>\n",
       "      <th>perf_6_month_avg</th>\n",
       "      <th>perf_12_month_avg</th>\n",
       "      <th>local_bo_qty</th>\n",
       "      <th>deck_risk</th>\n",
       "      <th>oe_constraint</th>\n",
       "      <th>ppap_risk</th>\n",
       "      <th>stop_auto_buy</th>\n",
       "      <th>rev_stop</th>\n",
       "      <th>went_on_backorder</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>606</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>846</th>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>882</th>\n",
       "      <td>-2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.34</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>898</th>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     national_inv  lead_time  in_transit_qty  forecast_3_month  \\\n",
       "197           0.0        2.0             0.0              54.0   \n",
       "606           0.0        2.0             0.0               2.0   \n",
       "846           1.0       12.0             0.0              18.0   \n",
       "882          -2.0        8.0             0.0              17.0   \n",
       "898           0.0        8.0             0.0              30.0   \n",
       "\n",
       "     forecast_6_month  forecast_9_month  sales_1_month  sales_3_month  \\\n",
       "197              54.0              54.0            0.0            0.0   \n",
       "606               2.0               2.0            3.0            3.0   \n",
       "846              18.0              18.0            1.0            2.0   \n",
       "882              23.0              33.0            2.0           12.0   \n",
       "898              45.0              60.0           11.0           26.0   \n",
       "\n",
       "     sales_6_month  sales_9_month  ...  pieces_past_due  perf_6_month_avg  \\\n",
       "197            0.0            0.0  ...              0.0              1.00   \n",
       "606            3.0            4.0  ...              0.0              0.73   \n",
       "846            8.0           15.0  ...              0.0              0.82   \n",
       "882           19.0           25.0  ...              0.0              0.37   \n",
       "898           42.0           73.0  ...              0.0              0.82   \n",
       "\n",
       "     perf_12_month_avg  local_bo_qty  deck_risk  oe_constraint  ppap_risk  \\\n",
       "197               0.98           0.0          1              0          1   \n",
       "606               0.77           0.0          0              0          0   \n",
       "846               0.79           0.0          0              0          0   \n",
       "882               0.34           2.0          0              0          0   \n",
       "898               0.81           0.0          0              0          0   \n",
       "\n",
       "     stop_auto_buy  rev_stop  went_on_backorder  \n",
       "197              1         0                  1  \n",
       "606              1         0                  1  \n",
       "846              1         0                  1  \n",
       "882              1         0                  1  \n",
       "898              1         0                  1  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = joblib.load('dataset_new.pkl')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['national_inv', 'lead_time', 'in_transit_qty', 'forecast_3_month',\n",
      "       'forecast_6_month', 'forecast_9_month', 'sales_1_month',\n",
      "       'sales_3_month', 'sales_6_month', 'sales_9_month', 'min_bank',\n",
      "       'potential_issue', 'pieces_past_due', 'perf_6_month_avg',\n",
      "       'perf_12_month_avg', 'local_bo_qty', 'deck_risk', 'oe_constraint',\n",
      "       'ppap_risk', 'stop_auto_buy', 'rev_stop', 'went_on_backorder'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##  Retrain a pipeline using the full sampled training data set\n",
    "\n",
    "Use the full sampled training data set to train the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add code below this comment  (Question #E301)\n",
    "# ----------------------------------\n",
    "\n",
    "\n",
    "X = df[[ 'national_inv', 'lead_time', 'in_transit_qty', 'forecast_3_month',\n",
    "       'forecast_6_month', 'forecast_9_month', 'sales_1_month',\n",
    "       'sales_3_month', 'sales_6_month', 'sales_9_month', 'min_bank',\n",
    "       'potential_issue', 'pieces_past_due', 'perf_6_month_avg',\n",
    "       'perf_12_month_avg', 'local_bo_qty', 'deck_risk', 'oe_constraint',\n",
    "       'ppap_risk', 'stop_auto_buy', 'rev_stop']]\n",
    "y = df[['went_on_backorder']]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#doing outlier detection as I wasn't able to save it in part-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lof = LocalOutlierFactor(n_neighbors=20, contamination=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model and make predictions\n",
    "y_pred_out = lof.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of outliers detected\n",
    "n_outliers = np.sum(y_pred_out == -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of outliers detected:  2256\n"
     ]
    }
   ],
   "source": [
    "# Print the number of outliers detected\n",
    "print(\"Number of outliers detected: \", n_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# retraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/feature_selection/_rfe.py:241: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X[:, features], y)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/feature_selection/_rfe.py:241: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X[:, features], y)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/feature_selection/_rfe.py:241: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X[:, features], y)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/feature_selection/_rfe.py:241: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X[:, features], y)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/feature_selection/_rfe.py:241: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X[:, features], y)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/feature_selection/_rfe.py:241: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X[:, features], y)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/feature_selection/_rfe.py:266: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self.estimator_.fit(X[:, features], y)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/pipeline.py:346: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params_last_step)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scale', MinMaxScaler()),\n",
       "                ('rfe',\n",
       "                 RFE(estimator=RandomForestClassifier(),\n",
       "                     n_features_to_select=15)),\n",
       "                ('rf', RandomForestClassifier(max_depth=20, n_estimators=200))])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_pipeline3.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the trained model with the pickle library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['retrain_model_3.pkl']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add code below this comment  \n",
    "# -----------------------------\n",
    "\n",
    "joblib.dump( best_pipeline3.fit , 'retrain_model_3.pkl')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Load the Testing Data and evaluate your model\n",
    "\n",
    " * `/dsa/data/all_datasets/back_order/Kaggle_Test_Dataset_v2.csv`\n",
    " \n",
    "* We need to preprocess this test data (**follow** the steps similar to Part I)\n",
    "* **If you have fitted any normalizer/standardizer in Part 2, then we have to transform this test data using the fitted normalizer/standardizer!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3058: DtypeWarning: Columns (0) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the given test set  (Question #E302)\n",
    "# ----------------------------------\n",
    "\n",
    "#read the dataset\n",
    "test_model = pd.read_csv(\"/dsa/data/all_datasets/back_order/Kaggle_Test_Dataset_v2.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocessing of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sku', 'potential_issue', 'deck_risk', 'oe_constraint', 'ppap_risk', 'stop_auto_buy', 'rev_stop', 'went_on_backorder']\n",
      "column  - sku has [3285085 3285131 3285358 ... '3526990' '3526991' '(242075 rows)']\n",
      "object\n",
      "column  - potential_issue has ['No' 'Yes' nan]\n",
      "object\n",
      "column  - deck_risk has ['Yes' 'No' nan]\n",
      "object\n",
      "column  - oe_constraint has ['No' 'Yes' nan]\n",
      "object\n",
      "column  - ppap_risk has ['No' 'Yes' nan]\n",
      "object\n",
      "column  - stop_auto_buy has ['Yes' 'No' nan]\n",
      "object\n",
      "column  - rev_stop has ['No' 'Yes' nan]\n",
      "object\n",
      "column  - went_on_backorder has ['No' 'Yes' nan]\n",
      "object\n",
      "float64\n",
      "(242076, 23)\n"
     ]
    }
   ],
   "source": [
    "#yes/no column\n",
    "yes_no_columns = list(filter(lambda i: test_model[i].dtype!=np.float64, test_model.columns))\n",
    "print(yes_no_columns)\n",
    "\n",
    "for i in (yes_no_columns):\n",
    "    a = test_model[i].unique()\n",
    "    print(\"column  - {} has {}\".format(i, a))\n",
    "    print(test_model['sku'].dtype)\n",
    "test_model['sku'] = pd.to_numeric(test_model['sku'], errors='coerce')\n",
    "print(test_model['sku'].dtype)\n",
    "print(test_model.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['potential_issue', 'deck_risk', 'oe_constraint', 'ppap_risk', 'stop_auto_buy', 'rev_stop', 'went_on_backorder']\n",
      "Filling missing values of potential_issue with No\n",
      "Filling missing values of deck_risk with No\n",
      "Filling missing values of oe_constraint with No\n",
      "Filling missing values of ppap_risk with No\n",
      "Filling missing values of stop_auto_buy with Yes\n",
      "Filling missing values of rev_stop with No\n",
      "Filling missing values of went_on_backorder with No\n",
      "potential_issue [0 1]\n",
      "deck_risk [1 0]\n",
      "oe_constraint [0 1]\n",
      "ppap_risk [0 1]\n",
      "stop_auto_buy [1 0]\n",
      "rev_stop [0 1]\n",
      "went_on_backorder [0 1]\n"
     ]
    }
   ],
   "source": [
    "#filling missing value\n",
    "    \n",
    "    \n",
    "yes_no_columns = list(filter(lambda i: test_model[i].dtype!=np.float64, test_model.columns))\n",
    "print(yes_no_columns)\n",
    "for column_name in yes_no_columns:\n",
    "    mode = test_model[column_name].apply(str).mode()[0]\n",
    "    print('Filling missing values of {} with {}'.format(column_name, mode))\n",
    "    test_model[column_name].fillna(mode, inplace=True)\n",
    "for i in yes_no_columns:\n",
    "    test_model[i].replace(('Yes', 'No'), (1, 0), inplace=True)\n",
    "    print(i, test_model[i].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model= test_model.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2604\n"
     ]
    }
   ],
   "source": [
    "print(test_model.went_on_backorder.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now predict and evaluate with the preprocessed test set. It would be interesting to see the performance with and without outliers removal from the test set. We can report confusion matrix, precision, recall, f1-score, accuracy, and other measures (if any). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add code below this comment  (Question #E303)\n",
    "# ----------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_test= test_model[['national_inv', 'lead_time', 'in_transit_qty', 'forecast_3_month',\n",
    "       'forecast_6_month', 'forecast_9_month', 'sales_1_month',\n",
    "       'sales_3_month', 'sales_6_month', 'sales_9_month', 'min_bank',\n",
    "       'potential_issue', 'pieces_past_due', 'perf_6_month_avg',\n",
    "       'perf_12_month_avg', 'local_bo_qty', 'deck_risk', 'oe_constraint',\n",
    "       'ppap_risk', 'stop_auto_buy', 'rev_stop']]\n",
    "y_test = test_model[[\"went_on_backorder\"]]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = test_model[[\"went_on_backorder\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model3.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[197304  27443]\n",
      " [   495   2109]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.88      0.93    224747\n",
      "           1       0.07      0.81      0.13      2604\n",
      "\n",
      "    accuracy                           0.88    227351\n",
      "   macro avg       0.53      0.84      0.53    227351\n",
      "weighted avg       0.99      0.88      0.92    227351\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Write a summary of your processing and an analysis of the model performance  \n",
    "# (Question #E304)\n",
    "# ----------------------------------\n",
    "\n",
    "results for confusion matrix:\n",
    "197200 -true negatives: These are the samples that were actually negative and were correctly classified as negative by the model.\n",
    "27547 -false positives: These are the samples that were actually negative but were incorrectly classified as positive by the model.\n",
    "502 -false negatives: These are the samples that were actually positive but were incorrectly classified as negative by the model.\n",
    "2102- true positives: These are the samples that were actually positive and were correctly classified as positive by the model.\n",
    "\n",
    "\n",
    "results from classification report:\n",
    "\n",
    "\n",
    "The accuracy of the model on the test set is 0.88, which means that 88% of the instances were correctly classified. The macro-averaged f1-score is 0.53, indicating that the model's performance is better for class 0 than for class 1. The weighted-average f1-score is 0.92, which is the weighted average of the f1-scores for each class, taking into account the number of instances for each class\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflect\n",
    "\n",
    "Imagine you are data scientist that has been tasked with developing a system to save your \n",
    "company money by predicting and preventing back orders of parts in the supply chain.\n",
    "\n",
    "Write a **brief summary** for \"management\" that details your findings, \n",
    "your level of certainty and trust in the models, \n",
    "and recommendations for operationalizing these models for the business."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Write your answer here:  \n",
    "# (Question #E305)\n",
    "# ----------------------------------\n",
    "\n",
    "\n",
    "\n",
    "Based on the investigation, I discovered that the data exhibits a considerable class imbalance although only a small portion of the items are backordered. Several models, including logistic regression, decision trees, and random forests, have been created and tested.\n",
    "\n",
    "The random forest model has the greatest accuracy of all the models I tested and better balance between precision and recall, at 0.88. The recall score for the positive class, on the other hand, is comparatively low, suggesting that the model still has trouble accurately identifying products that are out-of-stock. The imbalance in the data may be to blame for this.\n",
    "\n",
    "Prioritize the features of national inventory, in transit quantity, and sales in the last 3 months when predicting backorders.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save your notebook!\n",
    "## Then `File > Close and Halt`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
